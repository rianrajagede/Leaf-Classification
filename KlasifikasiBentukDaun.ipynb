{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage import data, io, filters, exposure\n",
    "from skimage.segmentation import slic, felzenszwalb\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import square, closing, binary_closing\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.measure import moments_hu, moments, moments_central, moments_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(name):\n",
    "    sample = io.imread(str(name))\n",
    "#     print(sample.shape)\n",
    "    sample = resize(sample, (148,224))\n",
    "#     plt.imshow(sample)\n",
    "#     print(sample.shape)\n",
    "\n",
    "    sample_hsv = color.convert_colorspace(sample,'RGB','HSV')\n",
    "#     print(sample_hsv[90,125])\n",
    "    for i in range(sample_hsv.shape[0]):\n",
    "        for j in range(sample_hsv.shape[1]):\n",
    "            if sample_hsv[i,j,0] > 0.2083 and sample_hsv[i,j,0] < 0.4167 and sample_hsv[i,j,2]>0.1  and sample_hsv[i,j,1]>0.1:\n",
    "                sample_hsv[i,j,1] = 0\n",
    "                sample_hsv[i,j,2] = 1\n",
    "            else:\n",
    "                sample_hsv[i,j,2] = 0\n",
    "#     print(sample_hsv[90,125])\n",
    "    sample_rgb = color.convert_colorspace(sample_hsv,'HSV','RGB')\n",
    "#     print(sample_rgb[90,125])\n",
    "    sample_grey = color.rgb2grey(sample_rgb)\n",
    "#     print(sample_grey[90,125])\n",
    "#     plt.imshow(sample_grey, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    t = 0\n",
    "    res_i = 0\n",
    "    res_j = 0\n",
    "    for i in range(sample_grey.shape[0]-100):\n",
    "        for j in range(sample_grey.shape[1]-100):\n",
    "            if np.sum(sample_grey[i:i+100,j:j+100]) > t:\n",
    "                res_i = i\n",
    "                res_j = j\n",
    "                t = np.sum(sample_grey[i:i+100,j:j+100])\n",
    "#     print(t, res_i, res_j)\n",
    "    sample_cropped = sample_grey[res_i:res_i+100,res_j:res_j+100]\n",
    "#     plt.imshow(sample_cropped, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    closed = binary_closing(sample_cropped, square(2))\n",
    "#     plt.imshow(closed, cmap=plt.get_cmap('gray'))\n",
    "    closed = np.asarray(closed, dtype=np.double)\n",
    "\n",
    "    M = moments(closed)\n",
    "    cr = M[1, 0] / M[0, 0]\n",
    "    cc = M[0, 1] / M[0, 0]\n",
    "#     print(cr,cc)\n",
    "    mu = moments_central(closed, cr=cr, cc=cc)\n",
    "    nu = moments_normalized(mu)\n",
    "    return moments_hu(nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "feature = np.empty((0,8), int)\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'train_1'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for o in onlyfiles:\n",
    "    feature = np.vstack((feature, np.hstack((get_feature(join(mypath,o)),0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "mypath = 'train_2'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for o in onlyfiles:\n",
    "    feature = np.vstack((feature,np.hstack((get_feature(join(mypath,o)),1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test = np.empty((0,8), int)\n",
    "mypath = 'test'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "x = 0\n",
    "for o in onlyfiles:\n",
    "    x = int(o[8:10])\n",
    "    if x<67:\n",
    "        u=0\n",
    "    else:\n",
    "        u=1\n",
    "    feature_test = np.vstack((feature_test,np.hstack((get_feature(join(mypath,o)),u))))\n",
    "    x += 1\n",
    "feature_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:358: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69875291\n",
      "Iteration 2, loss = 0.69715169\n",
      "Iteration 3, loss = 0.69575997\n",
      "Iteration 4, loss = 0.69455219\n",
      "Iteration 5, loss = 0.69350562\n",
      "Iteration 6, loss = 0.69260004\n",
      "Iteration 7, loss = 0.69181753\n",
      "Iteration 8, loss = 0.69114221\n",
      "Iteration 9, loss = 0.69056010\n",
      "Iteration 10, loss = 0.69005886\n",
      "Iteration 11, loss = 0.68962768\n",
      "Iteration 12, loss = 0.68925705\n",
      "Iteration 13, loss = 0.68893871\n",
      "Iteration 14, loss = 0.68866541\n",
      "Iteration 15, loss = 0.68843086\n",
      "Iteration 16, loss = 0.68822961\n",
      "Iteration 17, loss = 0.68805692\n",
      "Iteration 18, loss = 0.68790870\n",
      "Iteration 19, loss = 0.68778142\n",
      "Iteration 20, loss = 0.68767203\n",
      "Iteration 21, loss = 0.68757791\n",
      "Iteration 22, loss = 0.68749682\n",
      "Iteration 23, loss = 0.68742682\n",
      "Iteration 24, loss = 0.68736627\n",
      "Iteration 25, loss = 0.68731374\n",
      "Iteration 26, loss = 0.68726804\n",
      "Iteration 27, loss = 0.68722812\n",
      "Iteration 28, loss = 0.68719312\n",
      "Iteration 29, loss = 0.68716227\n",
      "Iteration 30, loss = 0.68713496\n",
      "Iteration 31, loss = 0.68711062\n",
      "Iteration 32, loss = 0.68708881\n",
      "Iteration 33, loss = 0.68706913\n",
      "Iteration 34, loss = 0.68705125\n",
      "Iteration 35, loss = 0.68703489\n",
      "Iteration 36, loss = 0.68701981\n",
      "Iteration 37, loss = 0.68700581\n",
      "Iteration 38, loss = 0.68699272\n",
      "Iteration 39, loss = 0.68698039\n",
      "Iteration 40, loss = 0.68696871\n",
      "Iteration 41, loss = 0.68695756\n",
      "Iteration 42, loss = 0.68694687\n",
      "Iteration 43, loss = 0.68693656\n",
      "Iteration 44, loss = 0.68692656\n",
      "Iteration 45, loss = 0.68691683\n",
      "Iteration 46, loss = 0.68690732\n",
      "Iteration 47, loss = 0.68689799\n",
      "Iteration 48, loss = 0.68688881\n",
      "Iteration 49, loss = 0.68687976\n",
      "Iteration 50, loss = 0.68687081\n",
      "Iteration 51, loss = 0.68686194\n",
      "Iteration 52, loss = 0.68685314\n",
      "Iteration 53, loss = 0.68684440\n",
      "Iteration 54, loss = 0.68683570\n",
      "Iteration 55, loss = 0.68682703\n",
      "Iteration 56, loss = 0.68681838\n",
      "Iteration 57, loss = 0.68680976\n",
      "Iteration 58, loss = 0.68680115\n",
      "Iteration 59, loss = 0.68679255\n",
      "Iteration 60, loss = 0.68678395\n",
      "Iteration 61, loss = 0.68677535\n",
      "Iteration 62, loss = 0.68676675\n",
      "Iteration 63, loss = 0.68675815\n",
      "Iteration 64, loss = 0.68674954\n",
      "Iteration 65, loss = 0.68674092\n",
      "Iteration 66, loss = 0.68673229\n",
      "Iteration 67, loss = 0.68672366\n",
      "Iteration 68, loss = 0.68671501\n",
      "Iteration 69, loss = 0.68670635\n",
      "Iteration 70, loss = 0.68669767\n",
      "Iteration 71, loss = 0.68668898\n",
      "Iteration 72, loss = 0.68668028\n",
      "Iteration 73, loss = 0.68667156\n",
      "Iteration 74, loss = 0.68666283\n",
      "Iteration 75, loss = 0.68665408\n",
      "Iteration 76, loss = 0.68664531\n",
      "Iteration 77, loss = 0.68663653\n",
      "Iteration 78, loss = 0.68662774\n",
      "Iteration 79, loss = 0.68661892\n",
      "Iteration 80, loss = 0.68661009\n",
      "Iteration 81, loss = 0.68660125\n",
      "Iteration 82, loss = 0.68659238\n",
      "Iteration 83, loss = 0.68658350\n",
      "Iteration 84, loss = 0.68657460\n",
      "Iteration 85, loss = 0.68656569\n",
      "Iteration 86, loss = 0.68655676\n",
      "Iteration 87, loss = 0.68654781\n",
      "Iteration 88, loss = 0.68653884\n",
      "Iteration 89, loss = 0.68652985\n",
      "Iteration 90, loss = 0.68652085\n",
      "Iteration 91, loss = 0.68651183\n",
      "Iteration 92, loss = 0.68650279\n",
      "Iteration 93, loss = 0.68649374\n",
      "Iteration 94, loss = 0.68648466\n",
      "Iteration 95, loss = 0.68647557\n",
      "Iteration 96, loss = 0.68646646\n",
      "Iteration 97, loss = 0.68645733\n",
      "Iteration 98, loss = 0.68644819\n",
      "Iteration 99, loss = 0.68643902\n",
      "Iteration 100, loss = 0.68642984\n",
      "Iteration 101, loss = 0.68642064\n",
      "Iteration 102, loss = 0.68641142\n",
      "Iteration 103, loss = 0.68640219\n",
      "Iteration 104, loss = 0.68639293\n",
      "Iteration 105, loss = 0.68638366\n",
      "Iteration 106, loss = 0.68637437\n",
      "Iteration 107, loss = 0.68636506\n",
      "Iteration 108, loss = 0.68635573\n",
      "Iteration 109, loss = 0.68634638\n",
      "Iteration 110, loss = 0.68633701\n",
      "Iteration 111, loss = 0.68632763\n",
      "Iteration 112, loss = 0.68631822\n",
      "Iteration 113, loss = 0.68630880\n",
      "Iteration 114, loss = 0.68629936\n",
      "Iteration 115, loss = 0.68628990\n",
      "Iteration 116, loss = 0.68628042\n",
      "Iteration 117, loss = 0.68627092\n",
      "Iteration 118, loss = 0.68626140\n",
      "Iteration 119, loss = 0.68625187\n",
      "Iteration 120, loss = 0.68624231\n",
      "Iteration 121, loss = 0.68623274\n",
      "Iteration 122, loss = 0.68622314\n",
      "Iteration 123, loss = 0.68621353\n",
      "Iteration 124, loss = 0.68620390\n",
      "Iteration 125, loss = 0.68619424\n",
      "Iteration 126, loss = 0.68618457\n",
      "Iteration 127, loss = 0.68617488\n",
      "Iteration 128, loss = 0.68616517\n",
      "Iteration 129, loss = 0.68615544\n",
      "Iteration 130, loss = 0.68614569\n",
      "Iteration 131, loss = 0.68613592\n",
      "Iteration 132, loss = 0.68612613\n",
      "Iteration 133, loss = 0.68611632\n",
      "Iteration 134, loss = 0.68610649\n",
      "Iteration 135, loss = 0.68609664\n",
      "Iteration 136, loss = 0.68608678\n",
      "Iteration 137, loss = 0.68607689\n",
      "Iteration 138, loss = 0.68606698\n",
      "Iteration 139, loss = 0.68605705\n",
      "Iteration 140, loss = 0.68604710\n",
      "Iteration 141, loss = 0.68603713\n",
      "Iteration 142, loss = 0.68602714\n",
      "Iteration 143, loss = 0.68601713\n",
      "Iteration 144, loss = 0.68600710\n",
      "Iteration 145, loss = 0.68599705\n",
      "Iteration 146, loss = 0.68598698\n",
      "Iteration 147, loss = 0.68597689\n",
      "Iteration 148, loss = 0.68596678\n",
      "Iteration 149, loss = 0.68595664\n",
      "Iteration 150, loss = 0.68594649\n",
      "Iteration 151, loss = 0.68593632\n",
      "Iteration 152, loss = 0.68592612\n",
      "Iteration 153, loss = 0.68591590\n",
      "Iteration 154, loss = 0.68590567\n",
      "Iteration 155, loss = 0.68589541\n",
      "Iteration 156, loss = 0.68588513\n",
      "Iteration 157, loss = 0.68587483\n",
      "Iteration 158, loss = 0.68586451\n",
      "Iteration 159, loss = 0.68585417\n",
      "Iteration 160, loss = 0.68584381\n",
      "Iteration 161, loss = 0.68583342\n",
      "Iteration 162, loss = 0.68582302\n",
      "Iteration 163, loss = 0.68581259\n",
      "Iteration 164, loss = 0.68580214\n",
      "Iteration 165, loss = 0.68579167\n",
      "Iteration 166, loss = 0.68578118\n",
      "Iteration 167, loss = 0.68577067\n",
      "Iteration 168, loss = 0.68576013\n",
      "Iteration 169, loss = 0.68574957\n",
      "Iteration 170, loss = 0.68573900\n",
      "Iteration 171, loss = 0.68572840\n",
      "Iteration 172, loss = 0.68571777\n",
      "Iteration 173, loss = 0.68570713\n",
      "Iteration 174, loss = 0.68569646\n",
      "Iteration 175, loss = 0.68568578\n",
      "Iteration 176, loss = 0.68567507\n",
      "Iteration 177, loss = 0.68566433\n",
      "Iteration 178, loss = 0.68565358\n",
      "Iteration 179, loss = 0.68564280\n",
      "Iteration 180, loss = 0.68563201\n",
      "Iteration 181, loss = 0.68562118\n",
      "Iteration 182, loss = 0.68561034\n",
      "Iteration 183, loss = 0.68559947\n",
      "Iteration 184, loss = 0.68558859\n",
      "Iteration 185, loss = 0.68557767\n",
      "Iteration 186, loss = 0.68556674\n",
      "Iteration 187, loss = 0.68555578\n",
      "Iteration 188, loss = 0.68554481\n",
      "Iteration 189, loss = 0.68553380\n",
      "Iteration 190, loss = 0.68552278\n",
      "Iteration 191, loss = 0.68551173\n",
      "Iteration 192, loss = 0.68550066\n",
      "Iteration 193, loss = 0.68548956\n",
      "Iteration 194, loss = 0.68547845\n",
      "Iteration 195, loss = 0.68546731\n",
      "Iteration 196, loss = 0.68545614\n",
      "Iteration 197, loss = 0.68544496\n",
      "Iteration 198, loss = 0.68543375\n",
      "Iteration 199, loss = 0.68542251\n",
      "Iteration 200, loss = 0.68541126\n",
      "Iteration 201, loss = 0.68539998\n",
      "Iteration 202, loss = 0.68538867\n",
      "Iteration 203, loss = 0.68537734\n",
      "Iteration 204, loss = 0.68536599\n",
      "Iteration 205, loss = 0.68535462\n",
      "Iteration 206, loss = 0.68534322\n",
      "Iteration 207, loss = 0.68533180\n",
      "Iteration 208, loss = 0.68532035\n",
      "Iteration 209, loss = 0.68530888\n",
      "Iteration 210, loss = 0.68529738\n",
      "Iteration 211, loss = 0.68528586\n",
      "Iteration 212, loss = 0.68527432\n",
      "Iteration 213, loss = 0.68526275\n",
      "Iteration 214, loss = 0.68525116\n",
      "Iteration 215, loss = 0.68523954\n",
      "Iteration 216, loss = 0.68522790\n",
      "Iteration 217, loss = 0.68521624\n",
      "Iteration 218, loss = 0.68520455\n",
      "Iteration 219, loss = 0.68519283\n",
      "Iteration 220, loss = 0.68518110\n",
      "Iteration 221, loss = 0.68516933\n",
      "Iteration 222, loss = 0.68515754\n",
      "Iteration 223, loss = 0.68514573\n",
      "Iteration 224, loss = 0.68513389\n",
      "Iteration 225, loss = 0.68512203\n",
      "Iteration 226, loss = 0.68511014\n",
      "Iteration 227, loss = 0.68509823\n",
      "Iteration 228, loss = 0.68508629\n",
      "Iteration 229, loss = 0.68507432\n",
      "Iteration 230, loss = 0.68506234\n",
      "Iteration 231, loss = 0.68505032\n",
      "Iteration 232, loss = 0.68503828\n",
      "Iteration 233, loss = 0.68502622\n",
      "Iteration 234, loss = 0.68501412\n",
      "Iteration 235, loss = 0.68500201\n",
      "Iteration 236, loss = 0.68498987\n",
      "Iteration 237, loss = 0.68497770\n",
      "Iteration 238, loss = 0.68496550\n",
      "Iteration 239, loss = 0.68495328\n",
      "Iteration 240, loss = 0.68494104\n",
      "Iteration 241, loss = 0.68492877\n",
      "Iteration 242, loss = 0.68491647\n",
      "Iteration 243, loss = 0.68490415\n",
      "Iteration 244, loss = 0.68489180\n",
      "Iteration 245, loss = 0.68487942\n",
      "Iteration 246, loss = 0.68486702\n",
      "Iteration 247, loss = 0.68485459\n",
      "Iteration 248, loss = 0.68484213\n",
      "Iteration 249, loss = 0.68482965\n",
      "Iteration 250, loss = 0.68481714\n",
      "Iteration 251, loss = 0.68480461\n",
      "Iteration 252, loss = 0.68479204\n",
      "Iteration 253, loss = 0.68477946\n",
      "Iteration 254, loss = 0.68476684\n",
      "Iteration 255, loss = 0.68475420\n",
      "Iteration 256, loss = 0.68474153\n",
      "Iteration 257, loss = 0.68472883\n",
      "Iteration 258, loss = 0.68471611\n",
      "Iteration 259, loss = 0.68470336\n",
      "Iteration 260, loss = 0.68469058\n",
      "Iteration 261, loss = 0.68467777\n",
      "Iteration 262, loss = 0.68466494\n",
      "Iteration 263, loss = 0.68465208\n",
      "Iteration 264, loss = 0.68463919\n",
      "Iteration 265, loss = 0.68462628\n",
      "Iteration 266, loss = 0.68461334\n",
      "Iteration 267, loss = 0.68460036\n",
      "Iteration 268, loss = 0.68458737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 269, loss = 0.68457434\n",
      "Iteration 270, loss = 0.68456129\n",
      "Iteration 271, loss = 0.68454820\n",
      "Iteration 272, loss = 0.68453509\n",
      "Iteration 273, loss = 0.68452195\n",
      "Iteration 274, loss = 0.68450879\n",
      "Iteration 275, loss = 0.68449559\n",
      "Iteration 276, loss = 0.68448237\n",
      "Iteration 277, loss = 0.68446912\n",
      "Iteration 278, loss = 0.68445584\n",
      "Iteration 279, loss = 0.68444253\n",
      "Iteration 280, loss = 0.68442919\n",
      "Iteration 281, loss = 0.68441583\n",
      "Iteration 282, loss = 0.68440243\n",
      "Iteration 283, loss = 0.68438901\n",
      "Iteration 284, loss = 0.68437555\n",
      "Iteration 285, loss = 0.68436207\n",
      "Iteration 286, loss = 0.68434856\n",
      "Iteration 287, loss = 0.68433502\n",
      "Iteration 288, loss = 0.68432145\n",
      "Iteration 289, loss = 0.68430786\n",
      "Iteration 290, loss = 0.68429423\n",
      "Iteration 291, loss = 0.68428057\n",
      "Iteration 292, loss = 0.68426688\n",
      "Iteration 293, loss = 0.68425317\n",
      "Iteration 294, loss = 0.68423942\n",
      "Iteration 295, loss = 0.68422565\n",
      "Iteration 296, loss = 0.68421184\n",
      "Iteration 297, loss = 0.68419801\n",
      "Iteration 298, loss = 0.68418414\n",
      "Iteration 299, loss = 0.68417025\n",
      "Iteration 300, loss = 0.68415632\n",
      "Iteration 301, loss = 0.68414237\n",
      "Iteration 302, loss = 0.68412838\n",
      "Iteration 303, loss = 0.68411437\n",
      "Iteration 304, loss = 0.68410032\n",
      "Iteration 305, loss = 0.68408625\n",
      "Iteration 306, loss = 0.68407214\n",
      "Iteration 307, loss = 0.68405800\n",
      "Iteration 308, loss = 0.68404384\n",
      "Iteration 309, loss = 0.68402964\n",
      "Iteration 310, loss = 0.68401541\n",
      "Iteration 311, loss = 0.68400115\n",
      "Iteration 312, loss = 0.68398686\n",
      "Iteration 313, loss = 0.68397254\n",
      "Iteration 314, loss = 0.68395818\n",
      "Iteration 315, loss = 0.68394380\n",
      "Iteration 316, loss = 0.68392938\n",
      "Iteration 317, loss = 0.68391494\n",
      "Iteration 318, loss = 0.68390046\n",
      "Iteration 319, loss = 0.68388595\n",
      "Iteration 320, loss = 0.68387141\n",
      "Iteration 321, loss = 0.68385684\n",
      "Iteration 322, loss = 0.68384223\n",
      "Iteration 323, loss = 0.68382760\n",
      "Iteration 324, loss = 0.68381293\n",
      "Iteration 325, loss = 0.68379823\n",
      "Iteration 326, loss = 0.68378350\n",
      "Iteration 327, loss = 0.68376873\n",
      "Iteration 328, loss = 0.68375394\n",
      "Iteration 329, loss = 0.68373911\n",
      "Iteration 330, loss = 0.68372425\n",
      "Iteration 331, loss = 0.68370936\n",
      "Iteration 332, loss = 0.68369443\n",
      "Iteration 333, loss = 0.68367947\n",
      "Iteration 334, loss = 0.68366448\n",
      "Iteration 335, loss = 0.68364946\n",
      "Iteration 336, loss = 0.68363440\n",
      "Iteration 337, loss = 0.68361932\n",
      "Iteration 338, loss = 0.68360419\n",
      "Iteration 339, loss = 0.68358904\n",
      "Iteration 340, loss = 0.68357385\n",
      "Iteration 341, loss = 0.68355863\n",
      "Iteration 342, loss = 0.68354338\n",
      "Iteration 343, loss = 0.68352809\n",
      "Iteration 344, loss = 0.68351277\n",
      "Iteration 345, loss = 0.68349742\n",
      "Iteration 346, loss = 0.68348203\n",
      "Iteration 347, loss = 0.68346661\n",
      "Iteration 348, loss = 0.68345115\n",
      "Iteration 349, loss = 0.68343567\n",
      "Iteration 350, loss = 0.68342014\n",
      "Iteration 351, loss = 0.68340459\n",
      "Iteration 352, loss = 0.68338900\n",
      "Iteration 353, loss = 0.68337337\n",
      "Iteration 354, loss = 0.68335771\n",
      "Iteration 355, loss = 0.68334202\n",
      "Iteration 356, loss = 0.68332630\n",
      "Iteration 357, loss = 0.68331053\n",
      "Iteration 358, loss = 0.68329474\n",
      "Iteration 359, loss = 0.68327891\n",
      "Iteration 360, loss = 0.68326304\n",
      "Iteration 361, loss = 0.68324714\n",
      "Iteration 362, loss = 0.68323121\n",
      "Iteration 363, loss = 0.68321524\n",
      "Iteration 364, loss = 0.68319924\n",
      "Iteration 365, loss = 0.68318320\n",
      "Iteration 366, loss = 0.68316712\n",
      "Iteration 367, loss = 0.68315101\n",
      "Iteration 368, loss = 0.68313487\n",
      "Iteration 369, loss = 0.68311869\n",
      "Iteration 370, loss = 0.68310247\n",
      "Iteration 371, loss = 0.68308622\n",
      "Iteration 372, loss = 0.68306993\n",
      "Iteration 373, loss = 0.68305361\n",
      "Iteration 374, loss = 0.68303725\n",
      "Iteration 375, loss = 0.68302086\n",
      "Iteration 376, loss = 0.68300443\n",
      "Iteration 377, loss = 0.68298797\n",
      "Iteration 378, loss = 0.68297146\n",
      "Iteration 379, loss = 0.68295493\n",
      "Iteration 380, loss = 0.68293835\n",
      "Iteration 381, loss = 0.68292174\n",
      "Iteration 382, loss = 0.68290509\n",
      "Iteration 383, loss = 0.68288841\n",
      "Iteration 384, loss = 0.68287169\n",
      "Iteration 385, loss = 0.68285493\n",
      "Iteration 386, loss = 0.68283814\n",
      "Iteration 387, loss = 0.68282131\n",
      "Iteration 388, loss = 0.68280444\n",
      "Iteration 389, loss = 0.68278754\n",
      "Iteration 390, loss = 0.68277060\n",
      "Iteration 391, loss = 0.68275362\n",
      "Iteration 392, loss = 0.68273660\n",
      "Iteration 393, loss = 0.68271955\n",
      "Iteration 394, loss = 0.68270246\n",
      "Iteration 395, loss = 0.68268533\n",
      "Iteration 396, loss = 0.68266817\n",
      "Iteration 397, loss = 0.68265096\n",
      "Iteration 398, loss = 0.68263372\n",
      "Iteration 399, loss = 0.68261644\n",
      "Iteration 400, loss = 0.68259913\n",
      "Iteration 401, loss = 0.68258177\n",
      "Iteration 402, loss = 0.68256438\n",
      "Iteration 403, loss = 0.68254695\n",
      "Iteration 404, loss = 0.68252948\n",
      "Iteration 405, loss = 0.68251197\n",
      "Iteration 406, loss = 0.68249443\n",
      "Iteration 407, loss = 0.68247684\n",
      "Iteration 408, loss = 0.68245922\n",
      "Iteration 409, loss = 0.68244156\n",
      "Iteration 410, loss = 0.68242386\n",
      "Iteration 411, loss = 0.68240612\n",
      "Iteration 412, loss = 0.68238834\n",
      "Iteration 413, loss = 0.68237053\n",
      "Iteration 414, loss = 0.68235267\n",
      "Iteration 415, loss = 0.68233477\n",
      "Iteration 416, loss = 0.68231684\n",
      "Iteration 417, loss = 0.68229887\n",
      "Iteration 418, loss = 0.68228085\n",
      "Iteration 419, loss = 0.68226280\n",
      "Iteration 420, loss = 0.68224471\n",
      "Iteration 421, loss = 0.68222658\n",
      "Iteration 422, loss = 0.68220840\n",
      "Iteration 423, loss = 0.68219019\n",
      "Iteration 424, loss = 0.68217194\n",
      "Iteration 425, loss = 0.68215365\n",
      "Iteration 426, loss = 0.68213532\n",
      "Iteration 427, loss = 0.68211695\n",
      "Iteration 428, loss = 0.68209853\n",
      "Iteration 429, loss = 0.68208008\n",
      "Iteration 430, loss = 0.68206159\n",
      "Iteration 431, loss = 0.68204305\n",
      "Iteration 432, loss = 0.68202448\n",
      "Iteration 433, loss = 0.68200587\n",
      "Iteration 434, loss = 0.68198721\n",
      "Iteration 435, loss = 0.68196851\n",
      "Iteration 436, loss = 0.68194978\n",
      "Iteration 437, loss = 0.68193100\n",
      "Iteration 438, loss = 0.68191218\n",
      "Iteration 439, loss = 0.68189332\n",
      "Iteration 440, loss = 0.68187442\n",
      "Iteration 441, loss = 0.68185547\n",
      "Iteration 442, loss = 0.68183649\n",
      "Iteration 443, loss = 0.68181746\n",
      "Iteration 444, loss = 0.68179839\n",
      "Iteration 445, loss = 0.68177928\n",
      "Iteration 446, loss = 0.68176013\n",
      "Iteration 447, loss = 0.68174094\n",
      "Iteration 448, loss = 0.68172170\n",
      "Iteration 449, loss = 0.68170242\n",
      "Iteration 450, loss = 0.68168310\n",
      "Iteration 451, loss = 0.68166374\n",
      "Iteration 452, loss = 0.68164434\n",
      "Iteration 453, loss = 0.68162489\n",
      "Iteration 454, loss = 0.68160540\n",
      "Iteration 455, loss = 0.68158587\n",
      "Iteration 456, loss = 0.68156629\n",
      "Iteration 457, loss = 0.68154668\n",
      "Iteration 458, loss = 0.68152701\n",
      "Iteration 459, loss = 0.68150731\n",
      "Iteration 460, loss = 0.68148756\n",
      "Iteration 461, loss = 0.68146778\n",
      "Iteration 462, loss = 0.68144794\n",
      "Iteration 463, loss = 0.68142807\n",
      "Iteration 464, loss = 0.68140815\n",
      "Iteration 465, loss = 0.68138818\n",
      "Iteration 466, loss = 0.68136818\n",
      "Iteration 467, loss = 0.68134813\n",
      "Iteration 468, loss = 0.68132803\n",
      "Iteration 469, loss = 0.68130790\n",
      "Iteration 470, loss = 0.68128771\n",
      "Iteration 471, loss = 0.68126749\n",
      "Iteration 472, loss = 0.68124722\n",
      "Iteration 473, loss = 0.68122690\n",
      "Iteration 474, loss = 0.68120655\n",
      "Iteration 475, loss = 0.68118614\n",
      "Iteration 476, loss = 0.68116570\n",
      "Iteration 477, loss = 0.68114521\n",
      "Iteration 478, loss = 0.68112467\n",
      "Iteration 479, loss = 0.68110409\n",
      "Iteration 480, loss = 0.68108346\n",
      "Iteration 481, loss = 0.68106279\n",
      "Iteration 482, loss = 0.68104208\n",
      "Iteration 483, loss = 0.68102132\n",
      "Iteration 484, loss = 0.68100051\n",
      "Iteration 485, loss = 0.68097966\n",
      "Iteration 486, loss = 0.68095877\n",
      "Iteration 487, loss = 0.68093782\n",
      "Iteration 488, loss = 0.68091684\n",
      "Iteration 489, loss = 0.68089581\n",
      "Iteration 490, loss = 0.68087473\n",
      "Iteration 491, loss = 0.68085360\n",
      "Iteration 492, loss = 0.68083243\n",
      "Iteration 493, loss = 0.68081122\n",
      "Iteration 494, loss = 0.68078996\n",
      "Iteration 495, loss = 0.68076865\n",
      "Iteration 496, loss = 0.68074730\n",
      "Iteration 497, loss = 0.68072590\n",
      "Iteration 498, loss = 0.68070445\n",
      "Iteration 499, loss = 0.68068296\n",
      "Iteration 500, loss = 0.68066142\n",
      "Iteration 501, loss = 0.68063983\n",
      "Iteration 502, loss = 0.68061820\n",
      "Iteration 503, loss = 0.68059652\n",
      "Iteration 504, loss = 0.68057480\n",
      "Iteration 505, loss = 0.68055303\n",
      "Iteration 506, loss = 0.68053121\n",
      "Iteration 507, loss = 0.68050934\n",
      "Iteration 508, loss = 0.68048742\n",
      "Iteration 509, loss = 0.68046546\n",
      "Iteration 510, loss = 0.68044346\n",
      "Iteration 511, loss = 0.68042140\n",
      "Iteration 512, loss = 0.68039930\n",
      "Iteration 513, loss = 0.68037714\n",
      "Iteration 514, loss = 0.68035495\n",
      "Iteration 515, loss = 0.68033270\n",
      "Iteration 516, loss = 0.68031041\n",
      "Iteration 517, loss = 0.68028806\n",
      "Iteration 518, loss = 0.68026567\n",
      "Iteration 519, loss = 0.68024323\n",
      "Iteration 520, loss = 0.68022075\n",
      "Iteration 521, loss = 0.68019821\n",
      "Iteration 522, loss = 0.68017563\n",
      "Iteration 523, loss = 0.68015300\n",
      "Iteration 524, loss = 0.68013032\n",
      "Iteration 525, loss = 0.68010759\n",
      "Iteration 526, loss = 0.68008481\n",
      "Iteration 527, loss = 0.68006199\n",
      "Iteration 528, loss = 0.68003911\n",
      "Iteration 529, loss = 0.68001619\n",
      "Iteration 530, loss = 0.67999321\n",
      "Iteration 531, loss = 0.67997019\n",
      "Iteration 532, loss = 0.67994712\n",
      "Iteration 533, loss = 0.67992400\n",
      "Iteration 534, loss = 0.67990083\n",
      "Iteration 535, loss = 0.67987761\n",
      "Iteration 536, loss = 0.67985434\n",
      "Iteration 537, loss = 0.67983103\n",
      "Iteration 538, loss = 0.67980766\n",
      "Iteration 539, loss = 0.67978424\n",
      "Iteration 540, loss = 0.67976077\n",
      "Iteration 541, loss = 0.67973726\n",
      "Iteration 542, loss = 0.67971369\n",
      "Iteration 543, loss = 0.67969007\n",
      "Iteration 544, loss = 0.67966640\n",
      "Iteration 545, loss = 0.67964269\n",
      "Iteration 546, loss = 0.67961892\n",
      "Iteration 547, loss = 0.67959510\n",
      "Iteration 548, loss = 0.67957123\n",
      "Iteration 549, loss = 0.67954731\n",
      "Iteration 550, loss = 0.67952334\n",
      "Iteration 551, loss = 0.67949932\n",
      "Iteration 552, loss = 0.67947525\n",
      "Iteration 553, loss = 0.67945113\n",
      "Iteration 554, loss = 0.67942695\n",
      "Iteration 555, loss = 0.67940273\n",
      "Iteration 556, loss = 0.67937846\n",
      "Iteration 557, loss = 0.67935413\n",
      "Iteration 558, loss = 0.67932975\n",
      "Iteration 559, loss = 0.67930532\n",
      "Iteration 560, loss = 0.67928084\n",
      "Iteration 561, loss = 0.67925631\n",
      "Iteration 562, loss = 0.67923173\n",
      "Iteration 563, loss = 0.67920709\n",
      "Iteration 564, loss = 0.67918241\n",
      "Iteration 565, loss = 0.67915767\n",
      "Iteration 566, loss = 0.67913288\n",
      "Iteration 567, loss = 0.67910804\n",
      "Iteration 568, loss = 0.67908314\n",
      "Iteration 569, loss = 0.67905820\n",
      "Iteration 570, loss = 0.67903320\n",
      "Iteration 571, loss = 0.67900815\n",
      "Iteration 572, loss = 0.67898305\n",
      "Iteration 573, loss = 0.67895789\n",
      "Iteration 574, loss = 0.67893268\n",
      "Iteration 575, loss = 0.67890742\n",
      "Iteration 576, loss = 0.67888211\n",
      "Iteration 577, loss = 0.67885675\n",
      "Iteration 578, loss = 0.67883133\n",
      "Iteration 579, loss = 0.67880586\n",
      "Iteration 580, loss = 0.67878034\n",
      "Iteration 581, loss = 0.67875476\n",
      "Iteration 582, loss = 0.67872913\n",
      "Iteration 583, loss = 0.67870345\n",
      "Iteration 584, loss = 0.67867771\n",
      "Iteration 585, loss = 0.67865192\n",
      "Iteration 586, loss = 0.67862608\n",
      "Iteration 587, loss = 0.67860019\n",
      "Iteration 588, loss = 0.67857424\n",
      "Iteration 589, loss = 0.67854824\n",
      "Iteration 590, loss = 0.67852218\n",
      "Iteration 591, loss = 0.67849607\n",
      "Iteration 592, loss = 0.67846991\n",
      "Iteration 593, loss = 0.67844369\n",
      "Iteration 594, loss = 0.67841742\n",
      "Iteration 595, loss = 0.67839110\n",
      "Iteration 596, loss = 0.67836472\n",
      "Iteration 597, loss = 0.67833829\n",
      "Iteration 598, loss = 0.67831180\n",
      "Iteration 599, loss = 0.67828526\n",
      "Iteration 600, loss = 0.67825867\n",
      "Iteration 601, loss = 0.67823202\n",
      "Iteration 602, loss = 0.67820532\n",
      "Iteration 603, loss = 0.67817856\n",
      "Iteration 604, loss = 0.67815175\n",
      "Iteration 605, loss = 0.67812488\n",
      "Iteration 606, loss = 0.67809796\n",
      "Iteration 607, loss = 0.67807099\n",
      "Iteration 608, loss = 0.67804396\n",
      "Iteration 609, loss = 0.67801687\n",
      "Iteration 610, loss = 0.67798973\n",
      "Iteration 611, loss = 0.67796254\n",
      "Iteration 612, loss = 0.67793529\n",
      "Iteration 613, loss = 0.67790799\n",
      "Iteration 614, loss = 0.67788063\n",
      "Iteration 615, loss = 0.67785321\n",
      "Iteration 616, loss = 0.67782575\n",
      "Iteration 617, loss = 0.67779822\n",
      "Iteration 618, loss = 0.67777064\n",
      "Iteration 619, loss = 0.67774301\n",
      "Iteration 620, loss = 0.67771532\n",
      "Iteration 621, loss = 0.67768757\n",
      "Iteration 622, loss = 0.67765977\n",
      "Iteration 623, loss = 0.67763191\n",
      "Iteration 624, loss = 0.67760400\n",
      "Iteration 625, loss = 0.67757604\n",
      "Iteration 626, loss = 0.67754801\n",
      "Iteration 627, loss = 0.67751993\n",
      "Iteration 628, loss = 0.67749180\n",
      "Iteration 629, loss = 0.67746361\n",
      "Iteration 630, loss = 0.67743536\n",
      "Iteration 631, loss = 0.67740706\n",
      "Iteration 632, loss = 0.67737870\n",
      "Iteration 633, loss = 0.67735029\n",
      "Iteration 634, loss = 0.67732182\n",
      "Iteration 635, loss = 0.67729329\n",
      "Iteration 636, loss = 0.67726471\n",
      "Iteration 637, loss = 0.67723607\n",
      "Iteration 638, loss = 0.67720738\n",
      "Iteration 639, loss = 0.67717863\n",
      "Iteration 640, loss = 0.67714982\n",
      "Iteration 641, loss = 0.67712096\n",
      "Iteration 642, loss = 0.67709204\n",
      "Iteration 643, loss = 0.67706306\n",
      "Iteration 644, loss = 0.67703403\n",
      "Iteration 645, loss = 0.67700494\n",
      "Iteration 646, loss = 0.67697579\n",
      "Iteration 647, loss = 0.67694659\n",
      "Iteration 648, loss = 0.67691733\n",
      "Iteration 649, loss = 0.67688802\n",
      "Iteration 650, loss = 0.67685865\n",
      "Iteration 651, loss = 0.67682922\n",
      "Iteration 652, loss = 0.67679973\n",
      "Iteration 653, loss = 0.67677019\n",
      "Iteration 654, loss = 0.67674059\n",
      "Iteration 655, loss = 0.67671094\n",
      "Iteration 656, loss = 0.67668122\n",
      "Iteration 657, loss = 0.67665145\n",
      "Iteration 658, loss = 0.67662163\n",
      "Iteration 659, loss = 0.67659174\n",
      "Iteration 660, loss = 0.67656180\n",
      "Iteration 661, loss = 0.67653181\n",
      "Iteration 662, loss = 0.67650175\n",
      "Iteration 663, loss = 0.67647164\n",
      "Iteration 664, loss = 0.67644147\n",
      "Iteration 665, loss = 0.67641125\n",
      "Iteration 666, loss = 0.67638096\n",
      "Iteration 667, loss = 0.67635062\n",
      "Iteration 668, loss = 0.67632022\n",
      "Iteration 669, loss = 0.67628977\n",
      "Iteration 670, loss = 0.67625926\n",
      "Iteration 671, loss = 0.67622869\n",
      "Iteration 672, loss = 0.67619806\n",
      "Iteration 673, loss = 0.67616738\n",
      "Iteration 674, loss = 0.67613664\n",
      "Iteration 675, loss = 0.67610584\n",
      "Iteration 676, loss = 0.67607498\n",
      "Iteration 677, loss = 0.67604407\n",
      "Iteration 678, loss = 0.67601310\n",
      "Iteration 679, loss = 0.67598207\n",
      "Iteration 680, loss = 0.67595099\n",
      "Iteration 681, loss = 0.67591985\n",
      "Iteration 682, loss = 0.67588865\n",
      "Iteration 683, loss = 0.67585739\n",
      "Iteration 684, loss = 0.67582607\n",
      "Iteration 685, loss = 0.67579470\n",
      "Iteration 686, loss = 0.67576327\n",
      "Iteration 687, loss = 0.67573178\n",
      "Iteration 688, loss = 0.67570024\n",
      "Iteration 689, loss = 0.67566864\n",
      "Iteration 690, loss = 0.67563698\n",
      "Iteration 691, loss = 0.67560526\n",
      "Iteration 692, loss = 0.67557349\n",
      "Iteration 693, loss = 0.67554165\n",
      "Iteration 694, loss = 0.67550976\n",
      "Iteration 695, loss = 0.67547782\n",
      "Iteration 696, loss = 0.67544581\n",
      "Iteration 697, loss = 0.67541375\n",
      "Iteration 698, loss = 0.67538163\n",
      "Iteration 699, loss = 0.67534945\n",
      "Iteration 700, loss = 0.67531722\n",
      "Iteration 701, loss = 0.67528493\n",
      "Iteration 702, loss = 0.67525258\n",
      "Iteration 703, loss = 0.67522017\n",
      "Iteration 704, loss = 0.67518771\n",
      "Iteration 705, loss = 0.67515518\n",
      "Iteration 706, loss = 0.67512260\n",
      "Iteration 707, loss = 0.67508997\n",
      "Iteration 708, loss = 0.67505727\n",
      "Iteration 709, loss = 0.67502452\n",
      "Iteration 710, loss = 0.67499171\n",
      "Iteration 711, loss = 0.67495885\n",
      "Iteration 712, loss = 0.67492592\n",
      "Iteration 713, loss = 0.67489294\n",
      "Iteration 714, loss = 0.67485991\n",
      "Iteration 715, loss = 0.67482681\n",
      "Iteration 716, loss = 0.67479366\n",
      "Iteration 717, loss = 0.67476045\n",
      "Iteration 718, loss = 0.67472718\n",
      "Iteration 719, loss = 0.67469386\n",
      "Iteration 720, loss = 0.67466047\n",
      "Iteration 721, loss = 0.67462704\n",
      "Iteration 722, loss = 0.67459354\n",
      "Iteration 723, loss = 0.67455999\n",
      "Iteration 724, loss = 0.67452638\n",
      "Iteration 725, loss = 0.67449271\n",
      "Iteration 726, loss = 0.67445899\n",
      "Iteration 727, loss = 0.67442521\n",
      "Iteration 728, loss = 0.67439137\n",
      "Iteration 729, loss = 0.67435747\n",
      "Iteration 730, loss = 0.67432352\n",
      "Iteration 731, loss = 0.67428951\n",
      "Iteration 732, loss = 0.67425545\n",
      "Iteration 733, loss = 0.67422133\n",
      "Iteration 734, loss = 0.67418715\n",
      "Iteration 735, loss = 0.67415291\n",
      "Iteration 736, loss = 0.67411862\n",
      "Iteration 737, loss = 0.67408427\n",
      "Iteration 738, loss = 0.67404987\n",
      "Iteration 739, loss = 0.67401541\n",
      "Iteration 740, loss = 0.67398089\n",
      "Iteration 741, loss = 0.67394632\n",
      "Iteration 742, loss = 0.67391169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 743, loss = 0.67387700\n",
      "Iteration 744, loss = 0.67384226\n",
      "Iteration 745, loss = 0.67380746\n",
      "Iteration 746, loss = 0.67377261\n",
      "Iteration 747, loss = 0.67373769\n",
      "Iteration 748, loss = 0.67370273\n",
      "Iteration 749, loss = 0.67366771\n",
      "Iteration 750, loss = 0.67363263\n",
      "Iteration 751, loss = 0.67359749\n",
      "Iteration 752, loss = 0.67356230\n",
      "Iteration 753, loss = 0.67352706\n",
      "Iteration 754, loss = 0.67349176\n",
      "Iteration 755, loss = 0.67345640\n",
      "Iteration 756, loss = 0.67342099\n",
      "Iteration 757, loss = 0.67338552\n",
      "Iteration 758, loss = 0.67335000\n",
      "Iteration 759, loss = 0.67331442\n",
      "Iteration 760, loss = 0.67327879\n",
      "Iteration 761, loss = 0.67324310\n",
      "Iteration 762, loss = 0.67320736\n",
      "Iteration 763, loss = 0.67317156\n",
      "Iteration 764, loss = 0.67313571\n",
      "Iteration 765, loss = 0.67309980\n",
      "Iteration 766, loss = 0.67306384\n",
      "Iteration 767, loss = 0.67302782\n",
      "Iteration 768, loss = 0.67299175\n",
      "Iteration 769, loss = 0.67295562\n",
      "Iteration 770, loss = 0.67291944\n",
      "Iteration 771, loss = 0.67288321\n",
      "Iteration 772, loss = 0.67284692\n",
      "Iteration 773, loss = 0.67281058\n",
      "Iteration 774, loss = 0.67277418\n",
      "Iteration 775, loss = 0.67273773\n",
      "Iteration 776, loss = 0.67270123\n",
      "Iteration 777, loss = 0.67266467\n",
      "Iteration 778, loss = 0.67262806\n",
      "Iteration 779, loss = 0.67259139\n",
      "Iteration 780, loss = 0.67255468\n",
      "Iteration 781, loss = 0.67251791\n",
      "Iteration 782, loss = 0.67248108\n",
      "Iteration 783, loss = 0.67244420\n",
      "Iteration 784, loss = 0.67240727\n",
      "Iteration 785, loss = 0.67237029\n",
      "Iteration 786, loss = 0.67233326\n",
      "Iteration 787, loss = 0.67229617\n",
      "Iteration 788, loss = 0.67225903\n",
      "Iteration 789, loss = 0.67222183\n",
      "Iteration 790, loss = 0.67218459\n",
      "Iteration 791, loss = 0.67214729\n",
      "Iteration 792, loss = 0.67210994\n",
      "Iteration 793, loss = 0.67207254\n",
      "Iteration 794, loss = 0.67203509\n",
      "Iteration 795, loss = 0.67199758\n",
      "Iteration 796, loss = 0.67196003\n",
      "Iteration 797, loss = 0.67192242\n",
      "Iteration 798, loss = 0.67188476\n",
      "Iteration 799, loss = 0.67184705\n",
      "Iteration 800, loss = 0.67180929\n",
      "Iteration 801, loss = 0.67177148\n",
      "Iteration 802, loss = 0.67173362\n",
      "Iteration 803, loss = 0.67169571\n",
      "Iteration 804, loss = 0.67165775\n",
      "Iteration 805, loss = 0.67161973\n",
      "Iteration 806, loss = 0.67158167\n",
      "Iteration 807, loss = 0.67154356\n",
      "Iteration 808, loss = 0.67150539\n",
      "Iteration 809, loss = 0.67146718\n",
      "Iteration 810, loss = 0.67142892\n",
      "Iteration 811, loss = 0.67139061\n",
      "Iteration 812, loss = 0.67135225\n",
      "Iteration 813, loss = 0.67131384\n",
      "Iteration 814, loss = 0.67127538\n",
      "Iteration 815, loss = 0.67123687\n",
      "Iteration 816, loss = 0.67119832\n",
      "Iteration 817, loss = 0.67115971\n",
      "Iteration 818, loss = 0.67112106\n",
      "Iteration 819, loss = 0.67108236\n",
      "Iteration 820, loss = 0.67104361\n",
      "Iteration 821, loss = 0.67100482\n",
      "Iteration 822, loss = 0.67096597\n",
      "Iteration 823, loss = 0.67092708\n",
      "Iteration 824, loss = 0.67088814\n",
      "Iteration 825, loss = 0.67084916\n",
      "Iteration 826, loss = 0.67081013\n",
      "Iteration 827, loss = 0.67077105\n",
      "Iteration 828, loss = 0.67073192\n",
      "Iteration 829, loss = 0.67069275\n",
      "Iteration 830, loss = 0.67065353\n",
      "Iteration 831, loss = 0.67061427\n",
      "Iteration 832, loss = 0.67057496\n",
      "Iteration 833, loss = 0.67053560\n",
      "Iteration 834, loss = 0.67049620\n",
      "Iteration 835, loss = 0.67045676\n",
      "Iteration 836, loss = 0.67041727\n",
      "Iteration 837, loss = 0.67037773\n",
      "Iteration 838, loss = 0.67033815\n",
      "Iteration 839, loss = 0.67029853\n",
      "Iteration 840, loss = 0.67025886\n",
      "Iteration 841, loss = 0.67021915\n",
      "Iteration 842, loss = 0.67017939\n",
      "Iteration 843, loss = 0.67013959\n",
      "Iteration 844, loss = 0.67009975\n",
      "Iteration 845, loss = 0.67005986\n",
      "Iteration 846, loss = 0.67001993\n",
      "Iteration 847, loss = 0.66997996\n",
      "Iteration 848, loss = 0.66993994\n",
      "Iteration 849, loss = 0.66989989\n",
      "Iteration 850, loss = 0.66985979\n",
      "Iteration 851, loss = 0.66981965\n",
      "Iteration 852, loss = 0.66977946\n",
      "Iteration 853, loss = 0.66973924\n",
      "Iteration 854, loss = 0.66969897\n",
      "Iteration 855, loss = 0.66965867\n",
      "Iteration 856, loss = 0.66961832\n",
      "Iteration 857, loss = 0.66957793\n",
      "Iteration 858, loss = 0.66953750\n",
      "Iteration 859, loss = 0.66949703\n",
      "Iteration 860, loss = 0.66945653\n",
      "Iteration 861, loss = 0.66941598\n",
      "Iteration 862, loss = 0.66937539\n",
      "Iteration 863, loss = 0.66933477\n",
      "Iteration 864, loss = 0.66929410\n",
      "Iteration 865, loss = 0.66925340\n",
      "Iteration 866, loss = 0.66921266\n",
      "Iteration 867, loss = 0.66917188\n",
      "Iteration 868, loss = 0.66913106\n",
      "Iteration 869, loss = 0.66909020\n",
      "Iteration 870, loss = 0.66904931\n",
      "Iteration 871, loss = 0.66900838\n",
      "Iteration 872, loss = 0.66896741\n",
      "Iteration 873, loss = 0.66892641\n",
      "Iteration 874, loss = 0.66888537\n",
      "Iteration 875, loss = 0.66884430\n",
      "Iteration 876, loss = 0.66880318\n",
      "Iteration 877, loss = 0.66876204\n",
      "Iteration 878, loss = 0.66872085\n",
      "Iteration 879, loss = 0.66867964\n",
      "Iteration 880, loss = 0.66863838\n",
      "Iteration 881, loss = 0.66859710\n",
      "Iteration 882, loss = 0.66855578\n",
      "Iteration 883, loss = 0.66851442\n",
      "Iteration 884, loss = 0.66847303\n",
      "Iteration 885, loss = 0.66843161\n",
      "Iteration 886, loss = 0.66839016\n",
      "Iteration 887, loss = 0.66834867\n",
      "Iteration 888, loss = 0.66830715\n",
      "Iteration 889, loss = 0.66826560\n",
      "Iteration 890, loss = 0.66822401\n",
      "Iteration 891, loss = 0.66818240\n",
      "Iteration 892, loss = 0.66814075\n",
      "Iteration 893, loss = 0.66809907\n",
      "Iteration 894, loss = 0.66805736\n",
      "Iteration 895, loss = 0.66801562\n",
      "Iteration 896, loss = 0.66797385\n",
      "Iteration 897, loss = 0.66793205\n",
      "Iteration 898, loss = 0.66789023\n",
      "Iteration 899, loss = 0.66784837\n",
      "Iteration 900, loss = 0.66780648\n",
      "Iteration 901, loss = 0.66776456\n",
      "Iteration 902, loss = 0.66772262\n",
      "Iteration 903, loss = 0.66768065\n",
      "Iteration 904, loss = 0.66763865\n",
      "Iteration 905, loss = 0.66759662\n",
      "Iteration 906, loss = 0.66755457\n",
      "Iteration 907, loss = 0.66751248\n",
      "Iteration 908, loss = 0.66747038\n",
      "Iteration 909, loss = 0.66742824\n",
      "Iteration 910, loss = 0.66738608\n",
      "Iteration 911, loss = 0.66734390\n",
      "Iteration 912, loss = 0.66730169\n",
      "Iteration 913, loss = 0.66725945\n",
      "Iteration 914, loss = 0.66721719\n",
      "Iteration 915, loss = 0.66717491\n",
      "Iteration 916, loss = 0.66713260\n",
      "Iteration 917, loss = 0.66709027\n",
      "Iteration 918, loss = 0.66704791\n",
      "Iteration 919, loss = 0.66700554\n",
      "Iteration 920, loss = 0.66696314\n",
      "Iteration 921, loss = 0.66692071\n",
      "Iteration 922, loss = 0.66687827\n",
      "Iteration 923, loss = 0.66683580\n",
      "Iteration 924, loss = 0.66679332\n",
      "Iteration 925, loss = 0.66675081\n",
      "Iteration 926, loss = 0.66670828\n",
      "Iteration 927, loss = 0.66666573\n",
      "Iteration 928, loss = 0.66662317\n",
      "Iteration 929, loss = 0.66658058\n",
      "Iteration 930, loss = 0.66653797\n",
      "Iteration 931, loss = 0.66649535\n",
      "Iteration 932, loss = 0.66645270\n",
      "Iteration 933, loss = 0.66641004\n",
      "Iteration 934, loss = 0.66636736\n",
      "Iteration 935, loss = 0.66632467\n",
      "Iteration 936, loss = 0.66628195\n",
      "Iteration 937, loss = 0.66623922\n",
      "Iteration 938, loss = 0.66619648\n",
      "Iteration 939, loss = 0.66615372\n",
      "Iteration 940, loss = 0.66611094\n",
      "Iteration 941, loss = 0.66606815\n",
      "Iteration 942, loss = 0.66602534\n",
      "Iteration 943, loss = 0.66598252\n",
      "Iteration 944, loss = 0.66593968\n",
      "Iteration 945, loss = 0.66589683\n",
      "Iteration 946, loss = 0.66585397\n",
      "Iteration 947, loss = 0.66581109\n",
      "Iteration 948, loss = 0.66576821\n",
      "Iteration 949, loss = 0.66572531\n",
      "Iteration 950, loss = 0.66568240\n",
      "Iteration 951, loss = 0.66563947\n",
      "Iteration 952, loss = 0.66559654\n",
      "Iteration 953, loss = 0.66555359\n",
      "Iteration 954, loss = 0.66551064\n",
      "Iteration 955, loss = 0.66546767\n",
      "Iteration 956, loss = 0.66542470\n",
      "Iteration 957, loss = 0.66538172\n",
      "Iteration 958, loss = 0.66533873\n",
      "Iteration 959, loss = 0.66529573\n",
      "Iteration 960, loss = 0.66525272\n",
      "Iteration 961, loss = 0.66520970\n",
      "Iteration 962, loss = 0.66516668\n",
      "Iteration 963, loss = 0.66512365\n",
      "Iteration 964, loss = 0.66508062\n",
      "Iteration 965, loss = 0.66503757\n",
      "Iteration 966, loss = 0.66499453\n",
      "Iteration 967, loss = 0.66495148\n",
      "Iteration 968, loss = 0.66490842\n",
      "Iteration 969, loss = 0.66486536\n",
      "Iteration 970, loss = 0.66482229\n",
      "Iteration 971, loss = 0.66477922\n",
      "Iteration 972, loss = 0.66473615\n",
      "Iteration 973, loss = 0.66469308\n",
      "Iteration 974, loss = 0.66465000\n",
      "Iteration 975, loss = 0.66460692\n",
      "Iteration 976, loss = 0.66456384\n",
      "Iteration 977, loss = 0.66452076\n",
      "Iteration 978, loss = 0.66447768\n",
      "Iteration 979, loss = 0.66443460\n",
      "Iteration 980, loss = 0.66439152\n",
      "Iteration 981, loss = 0.66434844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# 15 0.6 50\n",
    "# 9-3, 200, 0.1, 1500\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(9,3),\n",
    "                    solver='sgd',\n",
    "                    batch_size=200,\n",
    "                    activation='relu',\n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.1,\n",
    "                    max_iter=1500,\n",
    "                    momentum=0,\n",
    "                    tol=0.0,\n",
    "                    alpha=0,\n",
    "                    verbose=True,\n",
    "                    random_state=123)\n",
    "train_x = feature[:,:-1]\n",
    "train_y = feature[:,-1]\n",
    "test_x = feature_test[:,:-1]\n",
    "test_y = feature_test[:,-1]\n",
    "test_x.shape, test_y.shape\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "print(mlp.score(test_x,test_y))\n",
    "print(mlp.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
